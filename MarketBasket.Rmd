---
title: "Market Basket Analysis Instacart"
author: "Saurabh Chakrabarty & Priyanka Ghule"
output:
  html_document:
    theme: cosmo
---

```{r, message=FALSE, warning=FALSE}
library(readr)
library(dplyr)
library(tidyr)
library(arules)
library(arulesViz)
library(methods)

order_products__prior <- read_csv("../Input/order_products__prior.csv")
products <- read_csv("../Input/products.csv")
```

### Data Munging

```{r, message=FALSE}
# get the shopping baskets
order_baskets <- order_products__prior %>% 
  inner_join(products, by="product_id") %>% 
  group_by(order_id) %>%
  summarise(basket = as.vector(list(product_name)))

# compute transactions
transactions <- as(order_baskets$basket, "transactions")
```
### Analyzing the baskets

```{r}
hist(size(transactions), breaks = 0:150, xaxt="n", ylim=c(0,250000), 
     main = "Number of Items per basket", xlab = "Number of Items")
axis(1, at=seq(0,160,by=10), cex.axis=0.8)
mtext(paste("Total:", length(transactions), "baskets,", sum(size(transactions)), "items"))

#You can examine the distribution of transaction sizes

basketSizes<-size(transactions)
summary(basketSizes)
quantile(basketSizes, probs=seq(0,1,0.1))

```
To find out which products the customers are buying, we have to culculate the relative frequency of each product in the transaction data

```{r, fig.height=8}


item_frequencies <- itemFrequency(transactions)
sum(item_frequencies)
#Note that the frequencies don't sum to 1. You can recover the number of times that each product occurred in the data by normalizing the item frequencies and multiplying by the total number of items.

productCount <- (item_frequencies/sum(item_frequencies))*sum(basketSizes)
summary(productCount)

# Now sort the count and list the 10 most popular products in the basket
orderedProducts <- sort(productCount, decreasing=T)
orderedProducts[1:10]

orderedProducts[1]/dim(transactions)[1]

```
The most popular product in the dataset occurred in approx 15% of the baskets.

With mining high-dimensional data: almost every event is rare. You have to keep this point in mind when deciding on support thresholds for rule mining; your thresholds will often need to be quite low.

```{r}
support<-0.01
itemsets <- apriori(transactions, parameter = list(target = "frequent itemsets", supp=support, minlen=2), control = list(verbose = FALSE))

par(mar=c(5,18,2,2)+.1)
sets_order_supp <- DATAFRAME(sort(itemsets, by="support", decreasing = F))
barplot(sets_order_supp$support, names.arg=sets_order_supp$items, xlim=c(0,0.02), horiz = T, las = 2, cex.names = .8, main = "Frequent Itemsets")
mtext(paste("support:",support), padj = .8)
``` 
 
Let's try restricting the itemsets that we'll consider to those that are supported by at least 10000 baskets. This leads to a minimum support of: 0.0033 
```{r}
#Lets say we want to find products that occur together in each basket, we can't make any direct use of people who haven't yet shown interest in multiple products. 
#So we restrict the dataset to customers who have expressed interest in at least two products.

product_baskets <- transactions[basketSizes > 1]

10000/dim(product_baskets)[1] 

rules <- apriori(product_baskets, parameter = list(supp =  0.003269976, conf = 0.01, maxlen=3), control = list(verbose = FALSE)) 
plot(rules)

inspect(sort(rules, by="lift")[1:5])
inspect(sort(rules, by="confidence")[1:5])

```

**To be continued**